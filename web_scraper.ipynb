{"cells":[{"cell_type":"code","source":["years = list(range(1991,2023))"],"metadata":{"id":"4J3QUs91ljve"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OhDisRRQi2rQ"},"outputs":[],"source":["mvp_url = 'https://www.basketball-reference.com/awards/awards_{}.html'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XI-MLIUCi5aW","executionInfo":{"status":"ok","timestamp":1652986403983,"user_tz":240,"elapsed":7422,"user":{"displayName":"Ary Indarapu","userId":"05670538946318351224"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"70b4d724-259b-427c-d259-0e517356cc02"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/requests/__init__.py:91: RequestsDependencyWarning: urllib3 (1.26.9) or chardet (3.0.4) doesn't match a supported version!\n","  RequestsDependencyWarning)\n"]}],"source":["import requests\n","\n","# scrape basketball-reference.com for past 20 years\n","for year in years:\n","  url = mvp_url.format(year)\n","  data = requests.get(url)\n","\n","  # store html in raw_data folder\n","  with open('raw_data/mvp/{}.html'.format(year), 'w+') as f:\n","    f.write(data.text)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DyizKHBVk6oW"},"outputs":[],"source":["from bs4 import BeautifulSoup\n","import pandas as pd\n","\n","mvp_dfs = []\n","for year in years:\n","  with open('raw_data/mvp/{}.html'.format(year), 'r') as f: \n","    page = f.read()\n","  soup = BeautifulSoup(page, 'html.parser') # using bs4 to create parse class\n","\n","  soup.find('tr', class_='over_header').decompose()\n","  mvp_table = soup.find(id='mvp')\n","  mvp = pd.read_html(str(mvp_table))[0] # getting mvp table in dataframe\n","  mvp['Year'] = year\n","\n","  mvp_dfs.append(mvp)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A8SyJiODzFgm"},"outputs":[],"source":["# combine all data into csv file\n","mvps = pd.concat(mvp_dfs)\n","mvps.to_csv('mvps.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q5a3OdW97o3Z"},"outputs":[],"source":["# adding chrome webdriver for colab\n","import sys\n","sys.path.insert(0,'/usr/lib/chromium-browser/chromedriver')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RH7wJZe_8i0l"},"outputs":[],"source":["from selenium import webdriver\n","\n","# options to make sure colab doesn't crash\n","chrome_options = webdriver.ChromeOptions()\n","chrome_options.add_argument('--headless')\n","chrome_options.add_argument('--no-sandbox')\n","chrome_options.add_argument('--disable-dev-shm-usage')\n","\n","wd = webdriver.Chrome('chromedriver', options=chrome_options)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Cw6kE1N79VOU"},"outputs":[],"source":["player_stats_url = 'https://www.basketball-reference.com/leagues/NBA_{}_per_game.html'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EstkKLTs87Q7"},"outputs":[],"source":["import time\n","\n","# use selenium to scrape player stats with javascript\n","for year in years:\n","  url = player_stats_url.format(year)\n","  wd.get(url)\n","  wd.execute_script('window.scrollTo(1,10000)')\n","  time.sleep(2)\n","\n","  html = wd.page_source\n","  with open('raw_data/player_stats/{}.html'.format(year), 'w+') as f:\n","    f.write(html)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jB8dRJXe-k6q"},"outputs":[],"source":["player_dfs = []\n","\n","for year in years:\n","  with open('raw_data/player_stats/{}.html'.format(year)) as f:\n","    page = f.read()\n","  soup = BeautifulSoup(page, 'html.parser') # using bs4 to create parse class\n","\n","  soup.find('tr', class_='thead').decompose()\n","  player_table = soup.find(id='per_game_stats')\n","  player = pd.read_html(str(player_table))[0] # getting player table in dataframe\n","  player['Year'] = year\n","\n","  player_dfs.append(player)"]},{"cell_type":"code","source":["players = pd.concat(player_dfs)\n","players.to_csv('players.csv')"],"metadata":{"id":"GpBSH1YXGTpW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["team_stats_url = 'https://www.basketball-reference.com/leagues/NBA_{}_standings.html'"],"metadata":{"id":"8NIGn8F4K2cz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# scrape basketball-reference.com for past 20 years\n","for year in years:\n","  url = team_stats_url.format(year)\n","  data = requests.get(url)\n","\n","  # store html in raw_data folder\n","  with open('raw_data/team/{}.html'.format(year), 'w+') as f:\n","    f.write(data.text)"],"metadata":{"id":"7L88loiwPDFB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["team_dfs = []\n","for year in years:\n","  with open('raw_data/team/{}.html'.format(year), 'r') as f: \n","    page = f.read()\n","\n","  soup = BeautifulSoup(page, 'html.parser') # using bs4 to create parse class\n","  soup.find('tr', class_='thead').decompose()\n","\n","  # for eastern conference\n","  e_table = soup.find_all(id='divs_standings_E')[0]\n","  e_df = pd.read_html(str(e_table))[0] # getting team table in dataframe\n","  e_df['Year'] = year\n","  e_df['Team'] = e_df['Eastern Conference']\n","  del e_df['Eastern Conference']\n","  team_dfs.append(e_df)\n","\n","  # for western conference\n","  w_table = soup.find_all(id='divs_standings_W')[0]\n","  w_df = pd.read_html(str(w_table))[0] # getting team table in dataframe\n","  w_df['Year'] = year\n","  w_df['Team'] = w_df['Western Conference']\n","  del w_df['Western Conference']\n","  team_dfs.append(w_df)"],"metadata":{"id":"VNmgfubXPdvY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["teams = pd.concat(team_dfs)\n","teams.to_csv('teams.csv')"],"metadata":{"id":"i4MY-Go8QjIw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["advanced_url = 'https://www.basketball-reference.com/leagues/NBA_{}_advanced.html'"],"metadata":{"id":"Ug2EkMGRjs-2","executionInfo":{"status":"ok","timestamp":1653172128345,"user_tz":240,"elapsed":155,"user":{"displayName":"Ary Indarapu","userId":"05670538946318351224"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["import requests\n","\n","# scrape basketball-reference.com for past 20 years\n","for year in years:\n","  url = advanced_url.format(year)\n","  data = requests.get(url)\n","\n","  # store html in raw_data folder\n","  with open('raw_data/advanced_stats/{}.html'.format(year), 'w+') as f:\n","    f.write(data.text)"],"metadata":{"id":"E8zXujdclRuC","executionInfo":{"status":"ok","timestamp":1653172130759,"user_tz":240,"elapsed":1512,"user":{"displayName":"Ary Indarapu","userId":"05670538946318351224"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["from bs4 import BeautifulSoup\n","import pandas as pd\n","\n","adv_dfs = []\n","\n","for year in years:\n","  with open('raw_data/advanced_stats/{}.html'.format(year)) as f:\n","    page = f.read()\n","  soup = BeautifulSoup(page, 'html.parser') # using bs4 to create parse class\n","\n","  soup.find('tr', class_='thead').decompose()\n","  adv_table = soup.find(id='advanced_stats')\n","  adv = pd.read_html(str(adv_table))[0] # getting player table in dataframe\n","  adv['Year'] = year\n","\n","  adv_dfs.append(adv)"],"metadata":{"id":"Rnai6tHtj1wH","executionInfo":{"status":"ok","timestamp":1653172183130,"user_tz":240,"elapsed":52375,"user":{"displayName":"Ary Indarapu","userId":"05670538946318351224"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["advanced = pd.concat(adv_dfs)\n","advanced.to_csv('advanced_stats.csv')"],"metadata":{"id":"4xEzmA8rm1TP","executionInfo":{"status":"ok","timestamp":1653172183381,"user_tz":240,"elapsed":256,"user":{"displayName":"Ary Indarapu","userId":"05670538946318351224"}}},"execution_count":21,"outputs":[]}],"metadata":{"colab":{"name":"web_scraper.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPR/B6FsRWGzIpe7zIZ+hgT"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}